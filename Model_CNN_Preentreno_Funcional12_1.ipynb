{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielRondonGarcia/LSC/blob/main/Model_CNN_Preentreno_Funcional12_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  pip install wandb gputil pycm scikit-learn ipython Pillow tensorflow mlxtend pandas opencv-python tqdm torch seaborn scikit-learn scipy"
      ],
      "metadata": {
        "id": "8wPqZ9bvFkhm",
        "outputId": "a6029906-de67-4213-dc15-a14d9037a70b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Collecting pycm\n",
            "  Downloading pycm-3.6-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (7.9.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 56.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 47.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 54.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 47.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 55.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 58.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 56.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pycm) (1.21.6)\n",
            "Collecting art>=1.8\n",
            "  Downloading art-5.7-py2.py3-none-any.whl (592 kB)\n",
            "\u001b[K     |████████████████████████████████| 592 kB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython) (2.6.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 37.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython) (0.2.5)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0.7)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.4.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython) (0.7.0)\n",
            "Building wheels for collected packages: gputil, pathtools\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=e4fe9d66d3d0e0fade25cafd3cdf83c4d5e43af2fa39e6af313f78f7a06ec8ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=8c92377b5ac759d0534ca238fe2b7c6a5fdc4197ac25bb0f6024cd3ce80f82eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built gputil pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, jedi, GitPython, docker-pycreds, art, wandb, pycm, gputil\n",
            "Successfully installed GitPython-3.1.27 art-5.7 docker-pycreds-0.4.0 gitdb-4.0.9 gputil-1.4.0 jedi-0.18.1 pathtools-0.1.2 pycm-3.6 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N_wOpDwKESSe"
      },
      "outputs": [],
      "source": [
        "# Import libreries\n",
        "import zipfile\n",
        "import tarfile\n",
        "from six.moves import urllib\n",
        "from pycm import *\n",
        "import GPUtil\n",
        "import logging\n",
        "import sklearn\n",
        "from IPython.core.display import display, HTML\n",
        "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix, ConfusionMatrixDisplay\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.models import Model, load_model, Sequential, save_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, image, img_to_array\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import shutil\n",
        "import time\n",
        "import cv2 as cv2\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import os\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import wandb\n",
        "sns.set_style('darkgrid')\n",
        "# EXTRAS PRUEBAS\n",
        "# stop annoying tensorflow warning messages\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "%load_ext tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjW_OLHlESSm",
        "outputId": "eb89b3bf-cea1-45d2-93df-3d72ec70272c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "Num GPUs: 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\" config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.compat.v1.Session(config=config) \"\"\"\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs:\", len(physical_devices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP2rb6X0ESSn",
        "outputId": "acd99cb4-9ff7-4e40-e717-3306a1c4eecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 | 41% | 25% |\n"
          ]
        }
      ],
      "source": [
        "GPUtil.showUtilization()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yeOgucFHESSo",
        "outputId": "2a66a43c-afc2-41ab-cc49-30183454fdf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading LSC images from https://github.com/Kandriws/LSC/raw/main/Comprimido/Todas.tar...\n",
            "LSC photos are located in ./Todas\n"
          ]
        }
      ],
      "source": [
        "K.clear_session()\n",
        "data_dir = './Todas'\n",
        "path_master = './saved_model'\n",
        "path_master_exist = os.path.isdir(path_master)\n",
        "\n",
        "data_balance = True  # Traer CSV con paths de imagenes balanceadas\n",
        "\n",
        "if path_master_exist:\n",
        "    print('La carpeta saved_model existe.')\n",
        "else:\n",
        "    os.mkdir(path_master)\n",
        "exist_csv_with_split = os.path.isdir(data_dir)\n",
        "if not exist_csv_with_split:\n",
        "    DOWNLOAD_URL = 'https://github.com/Kandriws/LSC/raw/main/Comprimido/Todas.tar'\n",
        "    print('Downloading LSC images from %s...' % DOWNLOAD_URL)\n",
        "    urllib.request.urlretrieve(DOWNLOAD_URL, 'Todas.tar')\n",
        "    file_untar = 'Todas.tar'\n",
        "    untar = tarfile.TarFile(file_untar)\n",
        "    untar.extractall()\n",
        "    untar.close()\n",
        "    print('LSC photos are located in %s' % data_dir)\n",
        "\n",
        "split_test_data_exist = os.path.isfile(path_master+'/split_test_data.csv')\n",
        "split_train_data_exist = os.path.isfile(path_master+'/split_train_data.csv')\n",
        "split_val_data_exist = os.path.isfile(path_master+'/split_val_data.csv')\n",
        "if data_balance:\n",
        "    if not split_test_data_exist:\n",
        "        DOWNLOAD_URL = 'https://raw.githubusercontent.com/Kandriws/LSC/main/saved_model/split_test_data.csv'\n",
        "        urllib.request.urlretrieve(\n",
        "            DOWNLOAD_URL, path_master+'/split_test_data.csv')\n",
        "    if not split_train_data_exist:\n",
        "        DOWNLOAD_URL = 'https://raw.githubusercontent.com/Kandriws/LSC/main/saved_model/split_train_data.csv'\n",
        "        urllib.request.urlretrieve(\n",
        "            DOWNLOAD_URL, path_master+'/split_train_data.csv')\n",
        "    if not split_val_data_exist:\n",
        "        DOWNLOAD_URL = 'https://raw.githubusercontent.com/Kandriws/LSC/main/saved_model/split_val_data.csv'\n",
        "        urllib.request.urlretrieve(\n",
        "            DOWNLOAD_URL, path_master+'/split_val_data.csv')\n",
        "\n",
        "\n",
        "\"\"\" HyperParametros \"\"\"\n",
        "create_filter = False\n",
        "epoch = 100\n",
        "width, height = 224, 224\n",
        "channels = 3\n",
        "batch_size = 5\n",
        "img_shape = (height, width, channels)\n",
        "img_size = (width, height)\n",
        "fil_conv_1 = 32\n",
        "fil_conv_2 = 64\n",
        "tamano_fil_conv_1 = (3, 3)\n",
        "tamano_fil_conv_2 = (2, 2)\n",
        "tamano_pool = (2, 2)\n",
        "lr = 0.001\n",
        "model_name = 'EfficientNetV2S'  # EfficientNetV2S\n",
        "datestring = datetime.datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
        "path_file = './saved_model/'+model_name+'-'+str(epoch)+'-'+datestring\n",
        "csv_name_split_train = 'split_train_data.csv'\n",
        "csv_name_split_val = 'split_val_data.csv'\n",
        "csv_name_split_test = 'split_test_data.csv'\n",
        "flag = True  # para balancear el dataset creando imagenes sinteticas False = inicializa, True = impide\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cN9yJS9EESSq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def scalar(img):\n",
        "    return img\n",
        "\n",
        "\n",
        "def processImage(image):\n",
        "    image = cv2.imread(image)\n",
        "    \"\"\" image = cv2.imread(image)\n",
        "    blurred = cv2.GaussianBlur(image, (3, 3), 0)\n",
        "    gray = cv2.cv2tColor(blurred, cv2.COLOR_BGR2GRAY)\n",
        "    grad_x = cv2.Sobel(gray, cv2.CV2_16SC1, 1, 0)\n",
        "    # Encuentra el gradiente en la dirección y\n",
        "    grad_y = cv2.Sobel(gray, cv2.CV_16SC1, 0, 1)\n",
        "    edge1 = cv2.Canny(grad_x, grad_y, 10, 100) \"\"\"\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "train_df_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=scalar,\n",
        "    horizontal_flip=True,\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    validation_split=0.2\n",
        ")\n",
        "test_df_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=scalar,\n",
        ")\n",
        "valid_df_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=scalar,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z2_XGChESSr"
      },
      "outputs": [],
      "source": [
        "def create_filter_canny(fpath_img, i):\n",
        "    img = cv2.imread(fpath_img)  # Read image\n",
        "    # Defining all the parameters\n",
        "    t_lower = 100  # Lower Threshold\n",
        "    t_upper = 200  # Upper threshold\n",
        "    aperture_size = 5  # Aperture size\n",
        "    L2Gradient = True  # Boolean\n",
        "\n",
        "    # Applying the Canny Edge filter\n",
        "    # with Aperture Size and L2Gradient\n",
        "    edge = cv2.Canny(img, t_lower, t_upper,\n",
        "                     apertureSize=aperture_size,\n",
        "                     L2gradient=L2Gradient)\n",
        "    return cv2.imwrite(fpath_img, edge)\n",
        "    \"\"\" return cv2.imwrite(str(i)+'_canny_'+fpath_img, edge) \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQQAj96NESSs"
      },
      "source": [
        "image = tf.keras.preprocessing.image.load_img(\n",
        "            fpath, target_size=(224, 224))\n",
        "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "        image = (image - 128.) / 128."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MNZKbrU4ESSu",
        "outputId": "fab23137-55ea-4134-f532-4c20782458b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       O\n",
            "1       O\n",
            "2       O\n",
            "3       O\n",
            "4       O\n",
            "       ..\n",
            "5434    I\n",
            "5435    I\n",
            "5436    I\n",
            "5437    I\n",
            "5438    I\n",
            "Name: labels, Length: 5439, dtype: object\n"
          ]
        }
      ],
      "source": [
        "array_label = os.listdir(data_dir)\n",
        "count_lbl = len(array_label)\n",
        "filepaths = []\n",
        "labels = []\n",
        "for lbl in array_label:\n",
        "    classpath = os.path.join(data_dir, lbl)\n",
        "    flist = os.listdir(classpath)\n",
        "    for f, i in zip(flist, range(len(flist))):\n",
        "        fpath = os.path.join(classpath, f)\n",
        "        labels.append(lbl)\n",
        "        if create_filter:\n",
        "            filepaths.append(create_filter_canny(fpath, i))\n",
        "        else:\n",
        "            filepaths.append(fpath)\n",
        "\n",
        "Fseries = pd.Series(filepaths, name='filepaths')\n",
        "Lseries = pd.Series(labels, name='labels')\n",
        "df = pd.concat([Fseries, Lseries], axis=1)\n",
        "print(df['labels'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Biwb26J9ESSv"
      },
      "outputs": [],
      "source": [
        "def create_split_tvt():\n",
        "    parent_path_file = Path(str(path_file)).parent\n",
        "    exist_csv_with_split = os.path.isfile(\n",
        "        (str(parent_path_file) + '/' + str(csv_name_split_train)))\n",
        "    if not exist_csv_with_split:\n",
        "        stratify_use = df['labels']\n",
        "        tamano_df = len(df)\n",
        "        train_size_parameter = .70\n",
        "        test_size_parameter = .2\n",
        "        strat = df['labels']\n",
        "\n",
        "        train_df, residue_df = train_test_split(\n",
        "            df, train_size=train_size_parameter, shuffle=True, random_state=123, stratify=strat)\n",
        "\n",
        "        strat_residue_df = residue_df['labels']\n",
        "        test_size_parameter_use = (float(tamano_df) * test_size_parameter)/1\n",
        "        train_size_test = (test_size_parameter_use*1)/len(strat_residue_df)\n",
        "        valid_df, test_df = train_test_split(\n",
        "            residue_df, train_size=train_size_test, shuffle=True, random_state=123, stratify=strat_residue_df)\n",
        "\n",
        "        print('train_df length: ', len(train_df), '  valid_df length: ',\n",
        "              len(valid_df), '  test_df length: ', len(test_df))\n",
        "        # print(train_df)\n",
        "        print('\\nBalance de los datos de entrenamiento: ')\n",
        "        print(train_df['labels'].value_counts(ascending=True))\n",
        "\n",
        "        if os.path.isdir(path_file):\n",
        "            print('La carpeta existe.')\n",
        "        else:\n",
        "            os.mkdir(path_file)\n",
        "        train_save_split = os.path.join(parent_path_file, csv_name_split_train)\n",
        "        train_df.to_csv(train_save_split, index=False)\n",
        "\n",
        "        val_save_split = os.path.join(parent_path_file, csv_name_split_val)\n",
        "        valid_df.to_csv(val_save_split, index=False)\n",
        "\n",
        "        test_save_split = os.path.join(parent_path_file, csv_name_split_test)\n",
        "        test_df.to_csv(test_save_split, index=False)\n",
        "\n",
        "        split_train = pd.read_csv(train_save_split)\n",
        "        split_val = pd.read_csv(val_save_split)\n",
        "        split_test = pd.read_csv(test_save_split)\n",
        "        return split_train, split_val, split_test\n",
        "    else:\n",
        "        train_save_split = os.path.join(parent_path_file, csv_name_split_train)\n",
        "        val_save_split = os.path.join(parent_path_file, csv_name_split_val)\n",
        "        test_save_split = os.path.join(parent_path_file, csv_name_split_test)\n",
        "        split_train = pd.read_csv(train_save_split)\n",
        "        split_val = pd.read_csv(val_save_split)\n",
        "        split_test = pd.read_csv(test_save_split)\n",
        "        return split_train, split_val, split_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mStcAnrESSx",
        "outputId": "e5eef46d-e241-4e04-97e1-724c23b59e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H    174\n",
            "E    187\n",
            "F    232\n",
            "P    249\n",
            "Y    250\n",
            "B    250\n",
            "V    250\n",
            "T    250\n",
            "Q    250\n",
            "L    250\n",
            "N    250\n",
            "A    251\n",
            "K    252\n",
            "U    252\n",
            "X    252\n",
            "O    253\n",
            "M    255\n",
            "W    256\n",
            "D    259\n",
            "I    260\n",
            "C    275\n",
            "R    282\n",
            "Name: labels, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.labels.value_counts(ascending=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWBJKss5ESSx",
        "outputId": "9d220162-1fca-4658-b8d0-a6e72f484674"
      },
      "outputs": [
        {
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 1 fields in line 27, saw 411\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32mc:\\wamp64\\www\\LSC\\Model_CNN_Preentreno_Funcional12_1.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000010?line=0'>1</a>\u001b[0m train_df, valid_df, test_df \u001b[39m=\u001b[39m create_split_tvt()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000010?line=1'>2</a>\u001b[0m \u001b[39m\"\"\" prueba =dict(df.labels.value_counts(ascending=True))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000010?line=2'>3</a>\u001b[0m \u001b[39mfor k,v in sorted(prueba.items()):\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000010?line=3'>4</a>\u001b[0m \u001b[39m    print(k,v-174) \"\"\"\u001b[39;00m\n",
            "\u001b[1;32mc:\\wamp64\\www\\LSC\\Model_CNN_Preentreno_Funcional12_1.ipynb Cell 9'\u001b[0m in \u001b[0;36mcreate_split_tvt\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000008?line=46'>47</a>\u001b[0m test_save_split \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(parent_path_file, csv_name_split_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000008?line=47'>48</a>\u001b[0m split_train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(train_save_split)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000008?line=48'>49</a>\u001b[0m split_val \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(val_save_split)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000008?line=49'>50</a>\u001b[0m split_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(test_save_split)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/wamp64/www/LSC/Model_CNN_Preentreno_Funcional12_1.ipynb#ch0000008?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m split_train, split_val, split_test\n",
            "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=664'>665</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=665'>666</a>\u001b[0m     dialect,\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=666'>667</a>\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=675'>676</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=676'>677</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=677'>678</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=679'>680</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=577'>578</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=579'>580</a>\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=580'>581</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
            "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=1251'>1252</a>\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=1252'>1253</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=1253'>1254</a>\u001b[0m     index, columns, col_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[0;32m   <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=1254'>1255</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/readers.py?line=1255'>1256</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=222'>223</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=223'>224</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=224'>225</a>\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=225'>226</a>\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/AnonimusXD/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=226'>227</a>\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
            "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\AnonimusXD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 27, saw 411\n"
          ]
        }
      ],
      "source": [
        "train_df, valid_df, test_df = create_split_tvt()\n",
        "\"\"\" prueba =dict(df.labels.value_counts(ascending=True))\n",
        "for k,v in sorted(prueba.items()):\n",
        "    print(k,v-174) \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwWWm5TeESSy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_aumentation_gen(train_df, train_df_datagen, flag):\n",
        "    if flag == False:\n",
        "        train_df_shuffle = sklearn.utils.shuffle(train_df)\n",
        "        new_filepaths = []\n",
        "        new_labels = []\n",
        "        i = 0\n",
        "        num_images = 0\n",
        "        balance_train_df = dict(train_df.labels.value_counts(ascending=True))\n",
        "        max_item = max(balance_train_df.values())\n",
        "        count_img = 1\n",
        "        val = 0\n",
        "        print(max_item)\n",
        "        for k, v in balance_train_df.items():\n",
        "            for imgs_path, lbl in train_df_shuffle.values:\n",
        "                if k == lbl:\n",
        "                    if (count_img+v <= max_item):\n",
        "                        img = os.path.join(imgs_path)\n",
        "                        imge = load_img(img)\n",
        "                        imge = cv2.resize(image.img_to_array(imge), img_size,\n",
        "                                          interpolation=cv2.INTER_AREA)\n",
        "                        x = imge/255\n",
        "                        x = np.expand_dims(x, axis=0)\n",
        "                        for output_batch in train_df_datagen.flow(x, batch_size=1):\n",
        "                            a = image.img_to_array(output_batch[0])\n",
        "                            imagen = output_batch[0, :, :]*255\n",
        "                            imgfinal = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)\n",
        "                            img_save = (str(Path(imgs_path).parent)+\"/\"+lbl+\"_balance_%i%i.jpg\" %\n",
        "                                        (i, num_images))\n",
        "                            cv2.imwrite(img_save, imgfinal)\n",
        "                            num_images += 1\n",
        "                            new_filepaths.append(img_save)\n",
        "                            new_labels.append(lbl)\n",
        "                            count_img += 1\n",
        "                            break\n",
        "                    else:\n",
        "                        print(str(k)+' '+str(count_img-1))\n",
        "                        count_img = 1\n",
        "                        break\n",
        "            i += 1\n",
        "        Fseries_ = pd.Series(new_filepaths, name='filepaths')\n",
        "        Lseries_ = pd.Series(new_labels, name='labels')\n",
        "        train_df_temp = pd.concat([Fseries_, Lseries_], axis=1)\n",
        "        \"\"\" print(\"images generated\", num_images) \"\"\"\n",
        "        flag = True\n",
        "        return train_df.append(train_df_temp), flag\n",
        "    else:\n",
        "        return train_df, flag\n",
        "\n",
        "\n",
        "train_df, flag = data_aumentation_gen(train_df, train_df_datagen, flag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIbMPEjGESSz"
      },
      "outputs": [],
      "source": [
        "# color_mode='grayscale',\n",
        "img_train_gen = train_df_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col='filepaths',\n",
        "    y_col='labels',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    class_mode='categorical')\n",
        "img_val_gen = valid_df_datagen.flow_from_dataframe(\n",
        "    valid_df,\n",
        "    x_col='filepaths',\n",
        "    y_col='labels',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        "    class_mode='categorical')\n",
        "img_test_gen = test_df_datagen.flow_from_dataframe(\n",
        "    test_df,\n",
        "    x_col='filepaths',\n",
        "    y_col='labels',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        "    class_mode='categorical')\n",
        "\n",
        "classes = list(img_train_gen.class_indices.keys())\n",
        "print(classes)\n",
        "class_count = len(classes)\n",
        "train_steps = np.ceil(img_train_gen.n/batch_size)\n",
        "val_steps = np.ceil(img_val_gen.n/batch_size)\n",
        "test_steps = np.ceil(img_test_gen.n/batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLnWDAXoESS0"
      },
      "outputs": [],
      "source": [
        "def show_image_samples(gen):\n",
        "    t_dict = gen.class_indices\n",
        "    classes = list(t_dict.keys())\n",
        "    images, labels = next(gen)  # get a sample batch from the generator\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    length = len(labels)\n",
        "    if length < 25:  # show maximum of 25 images\n",
        "        r = length\n",
        "    else:\n",
        "        r = 25\n",
        "    for i in range(r):\n",
        "        plt.subplot(5, 5, i + 1)\n",
        "        image = images[i]/255\n",
        "        plt.imshow(image)\n",
        "        index = np.argmax(labels[i])\n",
        "        class_name = classes[index]\n",
        "        plt.title(class_name, color='blue', fontsize=12)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtjr0FrHESS0"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFNjXDJ4ESS1"
      },
      "outputs": [],
      "source": [
        "show_image_samples(img_train_gen)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhtBrjRQESS1"
      },
      "outputs": [],
      "source": [
        "\n",
        "cnn_base = tf.keras.applications.EfficientNetV2S(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=img_shape,\n",
        "    pooling='max',\n",
        ")\n",
        "x = cnn_base.output\n",
        "x = keras.layers.Flatten()(x)\n",
        "x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n",
        "x = Dense(256, kernel_regularizer=regularizers.l2(l=0.016), activity_regularizer=regularizers.l1(0.006),\n",
        "          bias_regularizer=regularizers.l1(0.006), activation='relu')(x)\n",
        "x = Dropout(rate=.45, seed=123)(x)\n",
        "output = Dense(class_count, activation='softmax')(x)\n",
        "model = Model(inputs=cnn_base.input, outputs=output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjhkwgLFESS2"
      },
      "source": [
        "sgd = optimizers.SGD(lr=1E-2, momentum=0.91,decay=5**(-4), nesterov=True)\n",
        "Y ADAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk7cCcx5ESS2"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(), metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mgt-gqmcESS3"
      },
      "outputs": [],
      "source": [
        "def my_learning_rate(epoch, lrate):\n",
        "    return lrate\n",
        "\n",
        "\n",
        "lrs = LearningRateScheduler(my_learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9huGCBQWESS3"
      },
      "outputs": [],
      "source": [
        "logdir = os.path.join(\n",
        "    path_file, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "cnn_fit = model.fit(img_train_gen,\n",
        "                    steps_per_epoch=train_steps,\n",
        "                    epochs=epoch,\n",
        "                    validation_data=img_val_gen,\n",
        "                    validation_steps=test_steps,\n",
        "                    callbacks=[tensorboard_callback, lrs],\n",
        "                    verbose=1,\n",
        "                    initial_epoch=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u0YpbDtESS4"
      },
      "outputs": [],
      "source": [
        "pd_model_history = pd.DataFrame(cnn_fit.history)\n",
        "log_name = str(\n",
        "    'log-'+datestring+'.xlsx')\n",
        "path_classification_report_csv = os.path.join(\n",
        "    path_file, log_name)\n",
        "pd_model_history.to_excel(\n",
        "    path_classification_report_csv, index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDyEQ5BAESS4"
      },
      "outputs": [],
      "source": [
        "bal = train_df['labels'].value_counts()\n",
        "fig, ax = plt.subplots(figsize=(16, 9))\n",
        "# Creamos Gráfica y ponesmos las barras de color verde\n",
        "  # oculta la cuadricula\n",
        "bar_train =ax.barh(bal.index, bal.values)\n",
        "ax.xaxis.set_tick_params(pad=5)\n",
        "ax.yaxis.set_tick_params(pad=10)\n",
        "plt.ylabel('Letters')\n",
        "plt.xlabel('Quantity')\n",
        "ax.bar_label(bar_train, padding=5)\n",
        "plt.title('Balance Train Dataset')\n",
        "plt.savefig(os.path.join(path_file, 'Balance_Train_Dataset.jpg'),\n",
        "                            dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OzBvMA1ESS5"
      },
      "outputs": [],
      "source": [
        "\n",
        "%tensorboard --logdir 'c:/Users/AnonimusXD/Documents/Model Proyecto de grado/saved_model'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyU01u5qESS5"
      },
      "outputs": [],
      "source": [
        "acc = cnn_fit.history['accuracy']\n",
        "val_acc = cnn_fit.history['val_accuracy']\n",
        "\n",
        "loss = cnn_fit.history['loss']\n",
        "val_loss = cnn_fit.history['val_loss']\n",
        "\n",
        "epochs_range = range(epoch)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.savefig(os.path.join(path_file, 'TA-VA-TL-VL.jpg'))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAfwQX1aESS6"
      },
      "outputs": [],
      "source": [
        "# Generate generalization metrics\n",
        "score = model.evaluate(img_test_gen, verbose=1)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V7EsJRuESS6"
      },
      "outputs": [],
      "source": [
        "# # Save the model\n",
        "name_model_save = str('model-'+model_name+'-'+datestring+'.h5')\n",
        "if os.path.isdir(path_file):\n",
        "    print('La carpeta existe.')\n",
        "else:\n",
        "    os.mkdir(path_file)\n",
        "filefolder_model = os.path.join(path_file, name_model_save)\n",
        "model.save(filefolder_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHnGCC6qESS7"
      },
      "outputs": [],
      "source": [
        "# # Save dataframe validation\n",
        "name_csv_df_val = str('temp-path-img-'+model_name+'-'+datestring+'.csv')\n",
        "filefolder_dfv = os.path.join(path_file, name_csv_df_val)\n",
        "test_df.to_csv(filefolder_dfv, encoding='utf-8', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUXU2LEIESS7"
      },
      "outputs": [],
      "source": [
        "# create csv with classes\n",
        "Class_series = pd.Series(classes, name='class')\n",
        "class_df = pd.concat([Class_series], axis=1)\n",
        "csv_name = 'class.csv'\n",
        "csv_save_class = os.path.join(path_file, csv_name)\n",
        "class_df.to_csv(csv_save_class, index=True, index_label='index')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewzCihBwESS7"
      },
      "outputs": [],
      "source": [
        "predicted = model.predict(img_test_gen, verbose=1)\n",
        "print_code = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4maytgO3ESS8"
      },
      "outputs": [],
      "source": [
        "def print_in_color(txt_msg, fore_tupple, back_tupple,):\n",
        "    # prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple\n",
        "    # text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n",
        "    rf, gf, bf = fore_tupple\n",
        "    rb, gb, bb = back_tupple\n",
        "    msg = '{0}' + txt_msg\n",
        "    mat = '\\33[38;2;' + str(rf) + ';' + str(gf) + ';' + str(bf) + \\\n",
        "        ';48;2;' + str(rb) + ';' + str(gb) + ';' + str(bb) + 'm'\n",
        "    print(msg .format(mat), flush=True)\n",
        "    print('\\33[0m', flush=True)  # returns default print color to back to black\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9hDUv2dESS8"
      },
      "outputs": [],
      "source": [
        "def plot_ci(cm, param, alpha=0.05, method=\"normal-approx\"):\n",
        "    \"\"\"\n",
        "    Plot two-sided confidence interval.\n",
        "\n",
        "    :param cm: ConfusionMatrix\n",
        "    :type cm : pycm.ConfusionMatrix object\n",
        "    :param param: input parameter\n",
        "    :type param: str\n",
        "    :param alpha: type I error\n",
        "    :type alpha: float\n",
        "    :param method: binomial confidence intervals method\n",
        "    :type method: str\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    conf_str = str(round(100*(1-alpha)))\n",
        "    print(conf_str+\"%CI :\")\n",
        "    if param in cm.class_stat.keys():\n",
        "        mean = []\n",
        "        error = [[], []]\n",
        "        data = cm.CI(param, alpha=alpha, binom_method=method)\n",
        "        class_names_str = list(map(str, (cm.classes)))\n",
        "        for class_index, class_name in enumerate(cm.classes):\n",
        "            \"\"\" print(str(class_name)+\" : \"+str(data[class_name][1])) \"\"\"\n",
        "            mean.append(cm.class_stat[param][class_name])\n",
        "            error[0].append(cm.class_stat[param][class_name] -\n",
        "                            data[class_name][1][0])\n",
        "            error[1].append(data[class_name][1][1] -\n",
        "                            cm.class_stat[param][class_name])\n",
        "        fig, ax = plt.subplots(figsize=(10, 22))\n",
        "        \"\"\" fig = plt.figure() \"\"\"\n",
        "        plt.errorbar(mean, class_names_str, xerr=error,\n",
        "                     fmt='o', capsize=5, linestyle=\"dotted\")\n",
        "        plt.ylabel('Class')\n",
        "        fig.suptitle(\"Param :\"+param + \", Alpha:\"+str(alpha), fontsize=16)\n",
        "        for index, value in enumerate(mean):\n",
        "            down_point = data[cm.classes[index]][1][0]\n",
        "            up_point = data[cm.classes[index]][1][1]\n",
        "            plt.text(value, class_names_str[index], \"%f\" %\n",
        "                     value, ha=\"center\", va=\"top\", color=\"red\")\n",
        "            plt.text(down_point, class_names_str[index], \"%f\" %\n",
        "                     down_point, ha=\"right\", va=\"bottom\", color=\"red\")\n",
        "            plt.text(up_point, class_names_str[index], \"%f\" %\n",
        "                     up_point, ha=\"left\", va=\"bottom\", color=\"red\")\n",
        "    else:\n",
        "        mean = cm.overall_stat[param]\n",
        "        data = cm.CI(param, alpha=alpha, binom_method=method)\n",
        "        print(data[1])\n",
        "        error = [[], []]\n",
        "        up_point = data[1][1]\n",
        "        down_point = data[1][0]\n",
        "        error[0] = [cm.overall_stat[param] - down_point]\n",
        "        error[1] = [up_point - cm.overall_stat[param]]\n",
        "        fig, ax = plt.subplots(figsize=(10, 22))\n",
        "        \"\"\" fig = plt.figure() \"\"\"\n",
        "        plt.errorbar(mean, [param], xerr=error, fmt='o',\n",
        "                     capsize=5, linestyle=\"dotted\")\n",
        "        fig.suptitle(\"Alpha:\"+str(alpha), fontsize=16)\n",
        "        plt.text(mean, param, \"%f\" % mean, ha=\"center\", va=\"top\", color=\"red\")\n",
        "        plt.text(down_point, param, \"%f\" % down_point,\n",
        "                 ha=\"right\", va=\"bottom\", color=\"red\")\n",
        "        plt.text(up_point, param, \"%f\" %\n",
        "                 up_point, ha=\"left\", va=\"bottom\", color=\"red\")\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_25KlS93ESS9"
      },
      "outputs": [],
      "source": [
        "def multiclass_roc_auc_score(y_test, y_pred, classes, average=\"macro\"):\n",
        "    fig, c_ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    y_pred = lb.transform(y_pred)\n",
        "\n",
        "    for (idx, c_label) in enumerate(classes):\n",
        "        fpr, tpr, thresholds = roc_curve(\n",
        "            y_test[:, idx].astype(int), y_pred[:, idx])\n",
        "        c_ax.plot(fpr, tpr, label='%s (AUC:%0.3f)' % (c_label, auc(fpr, tpr)))\n",
        "    c_ax.plot(fpr, fpr, 'k--', label='Random Guessing')\n",
        "    c_ax.annotate('Random Guess', (.5, .48), color='black')\n",
        "    c_ax.legend()\n",
        "    c_ax.set_xlabel('False Positive Rate')\n",
        "    c_ax.set_ylabel('True Positive Rate')\n",
        "    plt.savefig(os.path.join(path_file, 'multiclass_roc_auc_score.jpg'),\n",
        "                dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
        "    plt.show()\n",
        "    print('ROC AUC score:', roc_auc_score(y_test, y_pred, average=average))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_f0FhoVESS9"
      },
      "outputs": [],
      "source": [
        "def print_info(test_gen, preds):\n",
        "    class_dict = test_gen.class_indices\n",
        "    labels = test_gen.labels\n",
        "    file_names = test_gen.filenames\n",
        "    error_list = []\n",
        "    true_class = []\n",
        "    pred_class = []\n",
        "    prob_list = []\n",
        "    new_dict = {}\n",
        "    error_indices = []\n",
        "    y_pred = []\n",
        "    for key, value in class_dict.items():\n",
        "        # dictionary {integer of class number: string of class name}\n",
        "        new_dict[value] = key\n",
        "    # store new_dict as a text fine in the save_dir\n",
        "    classes = list(new_dict.values())     # list of string of class names\n",
        "    errors = 0\n",
        "    # False Negative\n",
        "    x_predic = []\n",
        "    for i, p in enumerate(preds):\n",
        "        pred_index = np.argmax(p)\n",
        "        x_predic.append(pred_index)\n",
        "        true_index = labels[i]  # labels are integer values\n",
        "        if pred_index != true_index:  # a misclassification has occurred\n",
        "            error_list.append(file_names[i])\n",
        "            true_class.append(new_dict[true_index])\n",
        "            pred_class.append(new_dict[pred_index])\n",
        "            prob_list.append(p[pred_index])\n",
        "            error_indices.append(true_index)\n",
        "            errors = errors + 1\n",
        "        y_pred.append(pred_index)\n",
        "\n",
        "    y_true = np.array(labels)\n",
        "    y_pred = np.array(y_pred)\n",
        "    if len(classes) <= 30:\n",
        "        # create a confusion matrix\n",
        "        cm2 = ConfusionMatrix(actual_vector=y_true, predict_vector=y_pred)\n",
        "        class_stat = pd.DataFrame(cm2.class_stat).transpose()\n",
        "        class_stat_csv = str(\n",
        "            'class_stat_'+datestring+'.xlsx')\n",
        "        class_stat_report_csv = os.path.join(\n",
        "            path_file, class_stat_csv)\n",
        "        class_stat.to_excel(\n",
        "            class_stat_report_csv, index=True)\n",
        "\n",
        "        fn_cm = cm2.FN\n",
        "        fp_cm = cm2.FP\n",
        "        tn_cm = cm2.TN\n",
        "        tp_cm = cm2.TP\n",
        "        count_fp = 0\n",
        "        count_fn = 0\n",
        "        count_tn = 0\n",
        "        count_tp = 0\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 2))\n",
        "        for k, v in fn_cm.items():\n",
        "            if v > 0:\n",
        "                bar_char_fn = ax.barh(classes[k], v, color='gray')\n",
        "                count_fn += v\n",
        "                plt.ylabel('Letters')\n",
        "                plt.xlabel('Quantity')\n",
        "                plt.title('False Negative = '+str(count_fn))\n",
        "                ax.bar_label(bar_char_fn, padding=5)\n",
        "                plt.savefig(os.path.join(path_file, 'False Negative.jpg'),\n",
        "                            dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 2))\n",
        "        for k, v in fp_cm.items():\n",
        "            if v > 0:\n",
        "                bar_char_fp = ax.barh(classes[k], v, color='gray')\n",
        "                count_fp += v\n",
        "                plt.ylabel('Letters')\n",
        "                plt.xlabel('Quantity')\n",
        "                plt.title('False Positive = '+str(count_fp))\n",
        "                ax.bar_label(bar_char_fp, padding=5)\n",
        "                plt.savefig(os.path.join(path_file, 'False Positive.jpg'),\n",
        "                            dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        for k, v in tn_cm.items():\n",
        "            if v > 0:\n",
        "                bar_char_tn = ax.barh(classes[k], v, color='gray')\n",
        "                count_tn += v\n",
        "                plt.ylabel('Letters')\n",
        "                plt.xlabel('Quantity')\n",
        "                plt.title('True Negative = '+str(count_tn))\n",
        "                ax.bar_label(bar_char_tn, padding=5)\n",
        "                plt.savefig(os.path.join(path_file, 'True Negative.jpg'),\n",
        "                            dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        for k, v in tp_cm.items():\n",
        "            if v > 0:\n",
        "                bar_char_tp = ax.barh(classes[k], v, color='gray')\n",
        "                count_tp += v\n",
        "                plt.ylabel('Letters')\n",
        "                plt.xlabel('Quantity')\n",
        "                plt.title('True Positive = '+str(count_tp))\n",
        "                ax.bar_label(bar_char_tp, padding=5)\n",
        "                plt.savefig(os.path.join(path_file, 'True Positive.jpg'),\n",
        "                            dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        length = len(classes)\n",
        "        if length < 8:\n",
        "            fig_width = 8\n",
        "            fig_height = 8\n",
        "        else:\n",
        "            fig_width = int(length * .5)\n",
        "            fig_height = int(length * .5)\n",
        "        plt.figure(figsize=(fig_width, fig_height))\n",
        "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues')\n",
        "        plt.xticks(np.arange(length)+.5, classes, rotation=90)\n",
        "        plt.yticks(np.arange(length)+.5, classes, rotation=0)\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"True Class\")\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.savefig(os.path.join(path_file, 'confusion_matrix.jpg'),\n",
        "                    dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
        "        plt.show()\n",
        "    clr = classification_report(\n",
        "        y_true, y_pred, target_names=classes, output_dict=True)\n",
        "\n",
        "    df_classification_report = pd.DataFrame(clr).transpose()\n",
        "\n",
        "    name_classification_report_csv = str(\n",
        "        'classification_report-'+datestring+'.xlsx')\n",
        "    path_classification_report_csv = os.path.join(\n",
        "        path_file, name_classification_report_csv)\n",
        "    df_classification_report.to_excel(\n",
        "        path_classification_report_csv, index=True)\n",
        "    \"\"\" print(\"Classification Report:\\n----------------------\\n\",\n",
        "          df_classification_report) \"\"\"\n",
        "    multiclass_roc_auc_score(y_true, x_predic, classes)\n",
        "\n",
        "\n",
        "print_info(img_test_gen, predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBIsu_b_ESS-"
      },
      "outputs": [],
      "source": [
        "def plot_image(predictions_array, class_df, true_label, img, i):\n",
        "    predictions_array, true_label = predictions_array, true_label\n",
        "    plt.figure(figsize=(20, 6))\n",
        "    grid = plt.GridSpec(1, 5, hspace=0.2, wspace=0.2)\n",
        "    plt.subplot(grid[0, 0])\n",
        "    plt.grid(b=None)\n",
        "    plt.imshow(img)\n",
        "    predicted_label = class_df.iloc[np.argmax(predictions_array)]\n",
        "    if predicted_label == true_label:\n",
        "        color = 'green'\n",
        "    else:\n",
        "        color = 'red'\n",
        "    plt.xlabel(\"{} {:6.2f}% ({})\".format(predicted_label,\n",
        "                                         100*np.max(predictions_array),\n",
        "                                         true_label),\n",
        "               color=color)\n",
        "    plt.subplot(grid[0, 1:])\n",
        "    plt.xticks(range(len(classes)), classes)\n",
        "    thisplot = plt.bar(range(len(classes)), predictions_array, color=\"#777777\")\n",
        "    ax.bar_label(thisplot, label_type='center')\n",
        "    plt.ylim([0, 1])\n",
        "    p_label = np.argmax(predictions_array)\n",
        "    thisplot[p_label].set_color('red')\n",
        "    thisplot[classes.index(true_label)].set_color('green')\n",
        "    i = 0\n",
        "    for p in thisplot:\n",
        "        width = p.get_width()\n",
        "        height = p.get_height()\n",
        "        x, y = p.get_xy()\n",
        "        plt.text(x+width/2,\n",
        "                 y+height*1.01,\n",
        "                 str(round(predictions_array[i]*100, 2))+'%',\n",
        "                 ha='center',\n",
        "                 weight='bold')\n",
        "        i += 1\n",
        "    if i < 5:\n",
        "        plt.savefig(os.path.join(path_file, str(i)+'_'+str(true_label)+'_predict_test.jpg'),\n",
        "                    dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5yP5SGrESS_"
      },
      "outputs": [],
      "source": [
        "def classify_with_model(i_, class_csv_path, filefolder_val_img,  model_path):\n",
        "    # read in the csv file\n",
        "    class_df = pd.read_csv(class_csv_path)\n",
        "    images_csv = pd.read_csv(filefolder_val_img)\n",
        "    crop_image = False\n",
        "    img_height = 224\n",
        "    img_width = 224\n",
        "    img_size_ = (img_width, img_height)\n",
        "    scale = 1\n",
        "    try:\n",
        "        s = int(scale)\n",
        "        s2 = 1\n",
        "        s1 = 0\n",
        "    except:\n",
        "        split = scale.split('-')\n",
        "        s1 = float(split[1])\n",
        "        s2 = float(split[0].split('*')[1])\n",
        "        print(s1, s2)\n",
        "    model = load_model(model_path)\n",
        "    for i in range(i_):\n",
        "        rand = random.randint(0, len(images_csv['filepaths']))\n",
        "        image_path = images_csv['filepaths'].iloc[rand]\n",
        "        class_img_path = images_csv['labels'].iloc[rand]\n",
        "        img = plt.imread(image_path)\n",
        "        img = cv2.resize(img, img_size_)\n",
        "        img_use = img\n",
        "        img = img*s2 - s1\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        p = np.squeeze(model.predict(img))\n",
        "        plot_image(p, class_df['class'], class_img_path, img_use, i)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzAVPVSVESS_"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def create_galery_val():\n",
        "    path_imgs = filefolder_dfv\n",
        "    images_csv = pd.read_csv(path_imgs)\n",
        "    filepath_img = []\n",
        "    labels_img = []\n",
        "    store_path = os.path.join(\n",
        "        path_file, 'storage-img-'+model_name+'-'+datestring)\n",
        "    os.mkdir(store_path)\n",
        "    for i in range(len(images_csv['filepaths'])):\n",
        "        image_path = images_csv['filepaths'].iloc[i]\n",
        "        class_img_path = images_csv['labels'].iloc[i]\n",
        "        img = cv2.imread(image_path)\n",
        "        file_name = os.path.split(image_path)[1]\n",
        "        dst_path = os.path.join(store_path, file_name)\n",
        "        filepath_img.append(dst_path)\n",
        "        labels_img.append(class_img_path)\n",
        "        cv2.imwrite(dst_path, img)\n",
        "    Fseries = pd.Series(filepath_img, name='filepaths')\n",
        "    Lseries = pd.Series(labels_img, name='labels')\n",
        "    df_img_val_new_file = pd.concat([Fseries, Lseries], axis=1)\n",
        "    name_csv_df_val = str('new-'+model_name+'-'+datestring+'.csv')\n",
        "    filefolder_val_img = os.path.join(path_file, name_csv_df_val)\n",
        "    df_img_val_new_file.to_csv(\n",
        "        filefolder_val_img, encoding='utf-8', index=False)\n",
        "    return store_path, filefolder_val_img\n",
        "\n",
        "\n",
        "store_path_img, filefolder_val_img = create_galery_val()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLaeEkgbESTA"
      },
      "outputs": [],
      "source": [
        "# save all path\n",
        "data_paths = {'model': [filefolder_model],\n",
        "              'class': [csv_save_class], 'img_val': [filefolder_val_img]}\n",
        "\n",
        "origin_paths = pd.DataFrame(data_paths)\n",
        "csv_name = 'origin_path.csv'\n",
        "csv_save_loc = os.path.join(path_file, csv_name)\n",
        "origin_paths.to_csv(csv_save_loc, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A-qiaq8ESTA"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "path_imgs = store_path_img\n",
        "class_csv_path = csv_save_class\n",
        "model_path = filefolder_model\n",
        "num_images = 5\n",
        "classify_with_model(num_images, class_csv_path,\n",
        "                    filefolder_val_img, model_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2930d103325ed71c6e2381c6daf3a88f175707bf3585d18173a8236fe5d58e8b"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}